{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPSfTSOm5xTxxmjXVvLoPSJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/waquasadnankarimi/Function/blob/main/KNN_%26_PCA_12.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 1: What is K-Nearest Neighbors (KNN) and how does it work in both\n",
        "classification and regression problems?\n",
        "\n",
        "Answer:\n",
        "- K-Nearest Neighbors (KNN) is a supervised machine learning algorithm that makes predictions based on the similarity (or distance) between data points. It assumes that similar data points exist close to each other in feature space.\n",
        "\n",
        "**How KNN Works (General Steps)**\n",
        "- Choose K: Select the number of neighbors (K) to consider, typically a small positive integer.\n",
        "- Calculate Distance: Compute the distance (e.g., Euclidean distance) between the new, unknown data point and all points in the training dataset.\n",
        "- Identify Neighbors: Find the K closest training points to the new point.\n",
        "Predict: Make a decision based on these K neighbors.\n",
        "\n",
        "**KNN for Classification**\n",
        "- Process: The algorithm identifies the K nearest neighbors, each with a continuous target value.\n",
        "- Prediction: The predicted value for the new point is the average (mean) of the target values of its K neighbors.\n",
        "- Example: Predicting house prices; the new house gets the average price of the K closest houses in the training data.\n",
        "\n",
        "**Key Characteristics**\n",
        "- Lazy Learning: Stores data instead of building a model, performing computation only during prediction.\n",
        "- Non-Parametric: Makes no assumptions about the data's underlying distribution.\n",
        "- Distance Metric: Crucial for defining \"closeness\" (e.g., Euclidean, Manhattan).\n",
        "- Feature Scaling: Important to standardize features before distance calculation to prevent bias.\n",
        "\n",
        "Question 2: What is the Curse of Dimensionality and how does it affect KNN\n",
        "performance?\n",
        "\n",
        "Answer:\n",
        "- The Curse of Dimensionality refers to problems that arise when the number of features (dimensions) in a dataset becomes very large. As dimensionality increases, data becomes sparse and distance-based algorithms struggle to find meaningful similarity.\n",
        "\n",
        "**What is the Curse of Dimensionality?**\n",
        "\n",
        "- Exponential Growth: The volume of the feature space increases exponentially with each added dimension, meaning data points become spread out (sparse).\n",
        "- Data Sparsity: With fixed data, the density of points decreases dramatically as dimensions rise, making it hard to find truly \"close\" neighbors.\n",
        "- Meaningless Distances: Distances between points become less discriminatory; the difference between the nearest and farthest neighbor diminishes, making all points seem equidistant.\n",
        "\n",
        "**How it affects KNN performance:**\n",
        "- Loss of Locality: The concept of \"nearest neighbors\" loses its meaning because points that seem close in high dimensions might not be meaningfully close in a practical sense.\n",
        "- Increased Computational Cost: Finding neighbors becomes computationally intensive and time-consuming.\n",
        "- Overfitting & Poor Generalization: With data spread thin, KNN can easily pick up noise, leading to models that perform well on training data but poorly on new data (overfitting).\n",
        "- Irrelevant Features: Extra, irrelevant features add noise, making it harder to find true patterns and further reducing accuracy.\n",
        "\n",
        "Question 3: What is Principal Component Analysis (PCA)? How is it different from\n",
        "feature selection?\n",
        "\n",
        "Answer:\n",
        "- Principal Component Analysis (PCA) is an unsupervised dimensionality reduction technique that transforms the original features into a new set of features called principal components. These components are linear combinations of the original features and are ordered such that the first few components retain most of the variance (information) in the data.\n",
        "\n",
        "**How PCA Works (Conceptually)**\n",
        "\n",
        "- Standardize the data\n",
        "- Compute covariance matrix\n",
        "- Find eigenvalues & eigenvectors\n",
        "- Select top components that explain most variance\n",
        "- Project data onto these components\n",
        "\n",
        "| Aspect               | PCA (Dimensionality Reduction)                | Feature Selection                     |\n",
        "| -------------------- | --------------------------------------------- | ------------------------------------- |\n",
        "| **Approach**         | Creates new transformed features              | Selects subset of existing features   |\n",
        "| **Nature**           | Feature extraction                            | Feature filtering                     |\n",
        "| **Interpretability** | Harder to interpret (components mix features) | Easy to interpret (original features) |\n",
        "| **Supervision**      | Unsupervised                                  | Can be supervised or unsupervised     |\n",
        "| **Output**           | Principal components (linear combinations)    | Original features (reduced set)       |\n",
        "| **Example**          | PC1 = 0.6x₁ + 0.4x₂ + …                       | Choose x₁, x₃, x₄ only                |\n",
        "\n",
        "**Intuitive Explanation**\n",
        "\n",
        "- PCA compresses the feature space by creating new axes\n",
        "- Feature selection removes unimportant features and keeps key ones\n",
        "\n",
        "Question 4: What are eigenvalues and eigenvectors in PCA, and why are they\n",
        "important?\n",
        "\n",
        "Answer:\n",
        "- In Principal Component Analysis (PCA), eigenvectors and eigenvalues are derived from the covariance matrix of the data and are used to transform high-dimensional data into a lower-dimensional space\n",
        "\n",
        "**1. What are Eigenvectors and Eigenvalues?**\n",
        "\n",
        "- Eigenvectors (Principal Components):\n",
        "  - They define the directions or axes of the new feature space.\n",
        "  - They are vectors that, when subjected to the linear transformation of the covariance matrix, do not change their direction, but only scale.\n",
        "  - In PCA, the first eigenvector points in the direction of maximum variance, and subsequent eigenvectors (which are orthogonal to the first) represent decreasing amounts of variance.\n",
        "- Eigenvalues (Magnitudes):\n",
        "  - They represent the scalar magnitude of the variance explained by each corresponding eigenvector.\n",
        "  - They indicate how much of the data's total variance (information) is captured by each principal component.\n",
        "  - A higher eigenvalue indicates that the corresponding eigenvector captures more information.\n",
        "\n",
        "**2. Why are They Important in PCA?**\n",
        "\n",
        "- Eigenvalues and eigenvectors are the core mechanism of PCA, making it possible to:\n",
        "\n",
        "  - Dimensionality Reduction: By calculating eigenvalues/eigenvectors, we can identify which directions (components) have high variance (large eigenvalues) and which have low variance (small eigenvalues). We can safely discard eigenvectors with small eigenvalues to reduce dimensions with minimal information loss.\n",
        "  - Maximized Information Retention: The top eigenvectors (principal components) are chosen based on the largest eigenvalues, ensuring that the most significant patterns and variability in the data are preserved.\n",
        "  - Decoupling Variables (Orthogonality): Eigenvectors are orthogonal (perpendicular) to each other, meaning the new components are uncorrelated. This eliminates multicollinearity, which is a common issue in high-dimensional datasets.\n",
        "  - Data Compression and Visualization: By reducing the number of dimensions to 2 or 3 principal components, high-dimensional datasets can be easily visualized and analyzed.\n",
        "  - Noise Reduction: Low eigenvalues often correspond to noise rather than signal. By eliminating these components, PCA acts as a filter to improve the signal-to-noise ratio.\n",
        "\n",
        "Question 5: How do KNN and PCA complement each other when applied in a single\n",
        "pipeline?\n",
        "\n",
        "Answer:\n",
        "- When applied in a single pipeline, PCA and KNN complement each other by addressing each other's limitations, primarily by transforming high-dimensional, noisy data into a lower-dimensional, cleaner space that enhances KNN's computational efficiency and predictive accuracy.\n",
        "\n",
        "**Here is how they complement each other**:\n",
        " - Solving the \"Curse of Dimensionality\": KNN relies on distance metrics (e.g., Euclidean distance) to find the nearest neighbors. In high-dimensional spaces, the distance between points becomes less meaningful, which degrades KNN performance. PCA reduces this dimensionality by projecting data into a lower-dimensional space, allowing KNN to function more effectively.\n",
        " - Improving Computational Efficiency: Because KNN calculates the distance from a test point to every training point, it is computationally expensive and slow on large, high-dimensional datasets. PCA reduces the number of features (dimensions), significantly speeding up the prediction time of the KNN model.\n",
        " - Noise Reduction and Data Cleaning: High-dimensional data often contains noise or redundant information that can trick KNN into picking wrong neighbors. PCA filters out this noise by focusing only on the components that explain the most variance, thus acting as a preprocessing step that improves classification accuracy.\n",
        " - Decoupling Features: PCA transforms raw, potentially correlated features into orthogonal (uncorrelated) principal components. This ensures that the distance metric used by KNN is not skewed by redundant or heavily correlated features.\n",
        "\n",
        "**Typical Pipeline**\n",
        "- A common ML pipeline looks like:\n",
        "  - Standardize features\n",
        "  - Apply PCA to reduce dimension\n",
        "  - Fit KNN classifier/regressor"
      ],
      "metadata": {
        "id": "tRbQEsXArj0a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Question 6: Train a KNN Classifier on the Wine dataset with and without feature\n",
        "scaling. Compare model accuracy in both cases.\n",
        "\n",
        "'''\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "data = load_wine()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
        "\n",
        "knn_no_scale = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_no_scale.fit(X_train, y_train)\n",
        "pred_no_scale = knn_no_scale.predict(X_test)\n",
        "acc_no_scale = accuracy_score(y_test, pred_no_scale)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "knn_scaled = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_scaled.fit(X_train_scaled, y_train)\n",
        "pred_scaled = knn_scaled.predict(X_test_scaled)\n",
        "acc_scaled = accuracy_score(y_test, pred_scaled)\n",
        "\n",
        "acc_no_scale, acc_scaled\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "43BTV_XeAMur",
        "outputId": "c3b5fe25-9b0b-46a8-c715-3c54942cf747"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.7222222222222222, 0.9444444444444444)"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Question 7: Train a PCA model on the Wine dataset and print the explained variance\n",
        "ratio of each principal component.\n",
        "'''\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "data = load_wine()\n",
        "X = data.data\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "pca = PCA()\n",
        "pca.fit(X_scaled)\n",
        "\n",
        "pca.explained_variance_ratio_\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "41QHfDNhAkxB",
        "outputId": "956b28b8-1035-45be-af06-ffb09cc13b0f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.36198848, 0.1920749 , 0.11123631, 0.0706903 , 0.06563294,\n",
              "       0.04935823, 0.04238679, 0.02680749, 0.02222153, 0.01930019,\n",
              "       0.01736836, 0.01298233, 0.00795215])"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Question 8: Train a KNN Classifier on the PCA-transformed dataset (retain top 2\n",
        "components). Compare the accuracy with the original dataset.\n",
        "'''\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "data = load_wine()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "knn_original = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_original.fit(X_train_scaled, y_train)\n",
        "pred_original = knn_original.predict(X_test_scaled)\n",
        "acc_original = accuracy_score(y_test, pred_original)\n",
        "\n",
        "pca = PCA(n_components=2)\n",
        "X_train_pca = pca.fit_transform(X_train_scaled)\n",
        "X_test_pca = pca.transform(X_test_scaled)\n",
        "\n",
        "knn_pca = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_pca.fit(X_train_pca, y_train)\n",
        "pred_pca = knn_pca.predict(X_test_pca)\n",
        "acc_pca = accuracy_score(y_test, pred_pca)\n",
        "\n",
        "acc_original, acc_pca\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zIZaKAVVAwXA",
        "outputId": "8af54758-7991-424c-a3b5-931d50006e37"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.9444444444444444, 0.9444444444444444)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 9: Train a KNN Classifier with different distance metrics (euclidean,\n",
        "manhattan) on the scaled Wine dataset and compare the results.\n",
        "\n",
        "Answe:\n",
        "\n",
        "- Training a K-Nearest Neighbors (KNN) classifier on the scaled Wine dataset involves preprocessing the 13 chemical features and evaluating the model using different distance metrics. For 2026, standard evaluations typically focus on Euclidean (straight-line) and Manhattan (grid-like) distances.\n",
        "\n",
        "**Training Procedure**\n",
        "- Scaling: The Wine dataset features have varying scales (e.g., alcohol content vs. color intensity). Feature scaling (like StandardScaler) is mandatory because KNN relies on distance; unscaled features with larger ranges would disproportionately influence results.\n",
        "- Training: Use the KNeighborsClassifier from scikit-learn.\n",
        "Euclidean Metric: Set metric='euclidean' (equivalent to \\(p=2\\) in Minkowski).Manhattan Metric: Set metric='manhattan' (equivalent to \\(p=1\\) in Minkowski)"
      ],
      "metadata": {
        "id": "Qgf9y5cdCE6i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Question 10: You are working with a high-dimensional gene expression dataset to\n",
        "classify patients with different types of cancer.\n",
        "Due to the large number of features and a small number of samples, traditional models\n",
        "overfit.\n",
        "Explain how you would:\n",
        "● Use PCA to reduce dimensionality\n",
        "● Decide how many components to keep\n",
        "● Use KNN for classification post-dimensionality reduction\n",
        "● Evaluate the model\n",
        "● Justify this pipeline to your stakeholders as a robust solution for real-world\n",
        "biomedical data\n",
        "'''\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.datasets import make_classification\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "X, y = make_classification(n_samples=100, n_features=1000, n_informative=50,\n",
        "                           n_classes=3, random_state=42)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "pca = PCA()\n",
        "pca.fit(X_scaled)\n",
        "\n",
        "cumulative_variance = np.cumsum(pca.explained_variance_ratio_)\n",
        "\n",
        "n_components = np.argmax(cumulative_variance >= 0.95) + 1\n",
        "print(f\"Number of components to explain 95% variance: {n_components}\")\n",
        "\n",
        "pca = PCA(n_components=n_components)\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_pca, y, test_size=0.3,\n",
        "                                                    random_state=42, stratify=y)\n",
        "knn = KNeighborsClassifier(n_neighbors=5)\n",
        "knn.fit(X_train, y_train)\n",
        "\n",
        "cv_scores = cross_val_score(knn, X_pca, y, cv=5)\n",
        "print(f\"Cross-Validation Accuracy: {cv_scores.mean():.2f} (+/- {cv_scores.std():.2f})\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GIxPkTdVC3QW",
        "outputId": "68f3b796-8935-4987-e8e6-25bdf0e2e81d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of components to explain 95% variance: 90\n",
            "Cross-Validation Accuracy: 0.43 (+/- 0.13)\n"
          ]
        }
      ]
    }
  ]
}