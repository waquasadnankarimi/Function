{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/waquasadnankarimi/Function/blob/main/Welcome_To_Colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 1 : What is Information Gain, and how is it used in Decision Trees?\n",
        "\n",
        "Answer-\n",
        "-  Entropy: A measure of impurity or disorder in a dataset (high entropy = mixed classes, low entropy = pure classes).\n",
        "\n",
        "-  Information Gain: The decrease in entropy achieved by splitting the data based on a specific attribute.\n",
        "\n",
        "- It quantifies the relevance of a feature for classification: a higher IG means the feature is more useful for distinguishing between classes.\n",
        "\n",
        "**How it's used in Decision Trees**\n",
        "- **Calculate Initial Entropy**: Determine the entropy of the entire dataset (root node).\n",
        "- **Evaluate Each Feature**: For every potential attribute (feature) to split on:\n",
        "  -  Calculate the entropy of each subset created by the split.\n",
        "  -  Calculate the weighted average entropy of the subsets.\n",
        "  - Subtract the weighted average entropy from the initial entropy to get the Information Gain for that attribute.\n",
        "- **Select the Best Split**: Choose the attribute with the highest Information Gain to be the node (or split) for that level of the tree.\n",
        "- **Repeat**: Continue this process recursively for each branch (subset) until leaf nodes are pure (low/zero entropy), resulting in a tree that efficiently classifies data by asking the most informative questions first.\n",
        "\n",
        "Question 2: What is the difference between Gini Impurity and Entropy?\n",
        "Hint: Directly compares the two main impurity measures, highlighting strengths,\n",
        "weaknesses, and appropriate use cases.\n",
        "\n",
        "Answer\n",
        "- Gini Impurity and Entropy are both metrics used to measure the impurity or disorder of a node in a decision tree, with the primary practical difference being computational efficiency and minor differences in how they select splits. Gini impurity is generally preferred due to its speed, while entropy is theoretically grounded in information theory.\n",
        "\n",
        "| **Feature**             | **Gini Impurity**                                                     | **Entropy (Information Gain)**                                                           |\n",
        "| ----------------------- | --------------------------------------------------------------------- | ---------------------------------------------------------------------------------------- |\n",
        "| **Calculation**         | Uses squared probability terms: (1 - \\sum p_i^2)                      | Uses logarithmic terms: (-\\sum p_i \\log_2(p_i))                                          |\n",
        "| **Range (Binary Case)** | (0 \\rightarrow 0.5) (0 = pure, 0.5 = maximally impure)                | (0 \\rightarrow 1.0) (0 = pure, 1 = maximally impure)                                     |\n",
        "| **Behavior**            | Favors splits that isolate the majority class (greedy)                | Produces more balanced splits and deeper trees                                           |\n",
        "| **Computational Cost**  | Faster, no log computations                                           | Slightly slower due to ( \\log ) operations                                               |\n",
        "| **Origin / Usage**      | Core impurity measure in **CART** (Classification & Regression Trees) | Derived from **Information Theory** for measuring uncertainty; used in **ID3/C4.5/C5.0** |\n",
        "\n",
        "**Strengths and Weaknesses**\n",
        "- Gini Impurity Strengths:\n",
        "  - Faster training: Its simpler calculation makes it the default choice when performance is critical, especially for large datasets.\n",
        "  - Robustness: Gini can be more robust to noise in highly dimensional data.\n",
        "  - Similar accuracy: In practice, it often yields very similar results to entropy.\n",
        "\n",
        "- Gini Impurity Weaknesses:\n",
        "  - Bias: It can be slightly biased toward selecting splits on features with a dominant class.\n",
        "   - Less sensitive: It is less sensitive to small changes in class probabilities compared to entropy.\n",
        "- Entropy Strengths:\n",
        "  - Theoretically sound: It is a more robust measure of information gain (reduction in uncertainty).\n",
        "  - Balanced splits: It may lead to more balanced and potentially deeper trees that capture subtle data structures.\n",
        "  - Slightly better results: While results are often similar, some studies suggest it can offer slightly better performance in specific cases.\n",
        "\n",
        "- Entropy Weaknesses:\n",
        "  - Slower training: The logarithmic operations increase training time, which is a significant downside for large-scale applications.\n",
        "  - Potential overfitting: Deeper, more complex trees may increase the risk of overfitting\n",
        "\n",
        "Question 3:What is Pre-Pruning in Decision Trees?\n",
        "\n",
        "Answe\n",
        "- Pre-pruning in decision trees, also known as early stopping, stops the tree from growing too complex during its construction by setting limits (like max depth, minimum samples per leaf, or minimum information gain), preventing overfitting the training data and improving generalization to new data, unlike post-pruning, which trims a fully grown tree. It's a preventative measure to build smaller, simpler trees from the start, making it computationally more efficient than building a full tree first\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "iID8EKB6chNG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Question 4:Write a Python program to train a Decision Tree Classifier using Gini\n",
        "Impurity as the criterion and print the feature importances (practical).\n",
        "Hint: Use criterion='gini' in DecisionTreeClassifier and access .\n",
        "feature_importances_.\n",
        "(Include your Python code and output in the code box below.)\n",
        "'''\n",
        "\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "import pandas as pd\n",
        "\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "clf = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "clf.fit(X, y)\n",
        "\n",
        "feature_importances = pd.Series(clf.feature_importances_, index=data.feature_names)\n",
        "\n",
        "print(\"Feature Importances:\")\n",
        "print(feature_importances)\n"
      ],
      "metadata": {
        "id": "xkyKzIkdhP7d",
        "outputId": "6386eba1-5a05-4122-fa2f-67378ceac40e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature Importances:\n",
            "sepal length (cm)    0.013333\n",
            "sepal width (cm)     0.000000\n",
            "petal length (cm)    0.564056\n",
            "petal width (cm)     0.422611\n",
            "dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 5: What is a Support Vector Machine (SVM)?\n",
        "\n",
        "Answer\n",
        "- A Support Vector Machine (SVM) is a powerful supervised machine learning algorithm used for classification and regression tasks\n",
        "\n",
        "Question 6: What is the Kernel Trick in SVM?\n",
        "\n",
        "Answer\n",
        "- The kernel trick in SVM is a powerful technique that allows Support Vector Machines to classify complex, non-linearly separable data by implicitly mapping it into a higher-dimensional space, where it becomes linearly separable, without the huge computational cost of actually performing the transformation\n",
        "\n"
      ],
      "metadata": {
        "id": "gKe0xsNfho1R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Question 7: Write a Python program to train two SVM classifiers with Linear\n",
        "and RBF kernels on the Wine dataset, then compare their accuracies.\n",
        "Hint:Use SVC(kernel='linear') and SVC(kernel='rbf'), then compare accuracy\n",
        "scores after fitting on the same dataset.\n",
        "(Include your Python code and output in the code box below.)\n",
        "'''\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "data = load_wine()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "svm_linear = SVC(kernel='linear')\n",
        "svm_linear.fit(X_train, y_train)\n",
        "\n",
        "svm_rbf = SVC(kernel='rbf')\n",
        "svm_rbf.fit(X_train, y_train)\n",
        "\n",
        "y_pred_linear = svm_linear.predict(X_test)\n",
        "y_pred_rbf = svm_rbf.predict(X_test)\n",
        "\n",
        "acc_linear = accuracy_score(y_test, y_pred_linear)\n",
        "acc_rbf = accuracy_score(y_test, y_pred_rbf)\n",
        "\n",
        "print(f\"Linear Kernel Accuracy: {acc_linear:.4f}\")\n",
        "print(f\"RBF Kernel Accuracy: {acc_rbf:.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "r9OewbKliS6O",
        "outputId": "e85d0bcc-b893-41bf-dfda-b01ecc6e7783",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Linear Kernel Accuracy: 1.0000\n",
            "RBF Kernel Accuracy: 0.8056\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 8: What is the Naïve Bayes classifier, and why is it called \"Naïve\"\n",
        "\n",
        "Answer\n",
        "- The Naïve Bayes classifier is a probabilistic machine learning algorithm based on Bayes’ Theorem that is commonly used for classification tasks, especially in text classification (e.g., spam detection, sentiment analysis)\n",
        "\n",
        "**Why is it called \"Naïve\"?**\n",
        " - The \"Naïve\" (or simple/idiot) part comes from its core assumption: conditional independence of features.\n",
        " - It assumes that the presence of one feature (like the word \"free\") doesn't affect the presence of another feature (like the word \"money\") within the same class (spam), given the class itself.\n",
        " - This assumption simplifies the complex calculations needed to find the joint probability of all features, making the algorithm computationally efficient.\n",
        " - While this independence is unrealistic for most real data (features often correlate), the algorithm still performs surprisingly well in practice, as demonstrated in many applications like\n",
        "\n",
        "Question 9: Explain the differences between Gaussian Naïve Bayes, Multinomial Naïve Bayes, and Bernoulli Naïve Bayes\n",
        "\n",
        "Answer\n",
        "- The core difference between the Naive Bayes variants lies in the assumptions they make about the distribution and type of the input features.\n",
        "\n",
        "- Gaussian Naïve Bayes:\n",
        "  - Data type: Used for continuous data, such as height, weight, or sensor measurements.\n",
        "  - Distribution assumption: It assumes that the continuous features follow a Gaussian (normal) distribution.\n",
        "- Multinomial Naïve Bayes:\n",
        "  - Data type: Designed for discrete data that represents counts, such as the frequency of words in a document.\n",
        "  - Distribution assumption: It assumes that the features are generated from a multinomial distribution.\n",
        "  - Key feature: It considers the count or frequency of occurrence of a feature.\n",
        "- Bernoulli Naïve Bayes:\n",
        "  - Data type: Suited for binary or boolean features, where each feature is either present (1) or absent (0).\n",
        "  - Distribution assumption: It uses the Bernoulli distribution.\n",
        "  - Key feature: It only cares about the presence or absence of a feature, not its frequency.\n",
        "\n",
        "| **Feature**                 | **Gaussian Naïve Bayes**                                   | **Multinomial Naïve Bayes**               | **Bernoulli Naïve Bayes**                     |\n",
        "| --------------------------- | ---------------------------------------------------------- | ----------------------------------------- | --------------------------------------------- |\n",
        "| **Data Type**               | Continuous numerical features                              | Discrete count data (integer frequencies) | Binary / Boolean values (0 or 1)              |\n",
        "| **Distribution Assumption** | Gaussian (Normal distribution)                             | Multinomial distribution                  | Bernoulli distribution                        |\n",
        "| **Typical Use Case**        | Predicting continuous attributes (e.g., height vs. weight) | Text classification based on word counts  | Spam detection based on word presence/absence |\n",
        "| **Common Domain**           | Medical / sensor data                                      | NLP text classification                   | Binary feature tasks (e.g., sentiment flags)  |\n",
        "| **Example Input**           | `[5.6, 73.2, 22.1]`                                        | `{word: count}`                           | `{word: present/not present}`                 |\n",
        "\n"
      ],
      "metadata": {
        "id": "71czXlNbi7T3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Question 10:\n",
        "Breast Cancer Dataset\n",
        "Write a Python program to train a Gaussian Naïve Bayes classifier on the Breast\n",
        "Cancer dataset and evaluate accuracy.\n",
        "Hint:Use GaussianNB() from sklearn.naive_bayes and the Breast Cancer dataset\n",
        "from sklearn.datasets.\n",
        "(Include your Python code and output in the code box below.\n",
        "'''\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "gnb = GaussianNB()\n",
        "gnb.fit(X_train, y_train)\n",
        "\n",
        "y_pred = gnb.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Gaussian Naïve Bayes Accuracy: {accuracy:.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "8_Xn7e5XkXDd",
        "outputId": "580b0440-f6d8-46bd-b659-25391d24a7a4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gaussian Naïve Bayes Accuracy: 0.9737\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome To Colab",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}