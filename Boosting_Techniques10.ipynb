{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM0+Clx2y463pxGYITl3WoS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/waquasadnankarimi/Function/blob/main/Boosting_Techniques10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 1: What is Boosting in Machine Learning? Explain how it improves weak\n",
        "learners.\n",
        "\n",
        "Answer:\n",
        "\n",
        "- Boosting in machine learning is an ensemble technique that sequentially combines multiple simple \"weak learners\" (models slightly better than random guessing) into one powerful \"strong learner\" by iteratively focusing on the mistakes of previous models, adjusting data point weights to emphasize misclassified samples, thus reducing bias and improving overall accuracy for complex problems.\n",
        "\n",
        "**How Boosting Works to Improve Weak Learners**\n",
        "- **Sequential Training**: Unlike bagging, boosting trains models one after another (sequentially), not in parallel.\n",
        "- **Error Correction Focus:** Each new weak learner (often a shallow decision tree) is trained to correct the errors or residuals (missed predictions) of the combined model from the previous steps.\n",
        "- **Weight Adjustment:** The algorithm increases the weight (importance) of data points that were misclassified by earlier models, forcing subsequent learners to pay more attention to these difficult examples.\n",
        "- **Weighted Combination:** The final strong model combines the predictions of all weak learners, often using a weighted majority vote (for classification) or sum (for regression), where better-performing learners have a greater say.\n",
        "\n",
        "Question 2: What is the difference between AdaBoost and Gradient Boosting in terms of how models are trained?\n",
        "\n",
        "Answer:\n",
        "- Both AdaBoost and Gradient Boosting are boosting algorithms, but they differ in how they train new learners and handle errors.\n",
        "\n",
        "**AdaBoost (Adaptive Boosting)**\n",
        "- Error Correction: Increases weights of misclassified data points, forcing the next model to focus on them.\n",
        "- Weak Learners: Typically uses simple decision stumps (trees with one split).\n",
        "- Model Updates: Assigns weights to classifiers based on performance; stronger learners get higher weights.\n",
        "- Focus: Adapting the data distribution to focus on difficult samples.\n",
        "\n",
        "**Gradient Boosting (GBM)**\n",
        "- Error Correction: Trains new models (trees) to predict the residuals (errors) of the previous model, moving in the direction of the steepest descent (gradient) of the loss function.\n",
        "- Weak Learners: Can use more complex learners, often deeper decision trees.\n",
        "- Model Updates: Uses a learning rate (shrinkage) to control the contribution of each tree, allowing for smaller, more stable updates.\n",
        "- Focus: Minimizing a specific, differentiable loss function (like squared error or log loss).\n",
        "\n",
        "**Key Difference Summary**\n",
        "- How Errors Are Handled: AdaBoost adjusts sample weights; Gradient Boosting adjusts model predictions by fitting residuals.\n",
        "- Model Complexity: AdaBoost uses simple stumps; Gradient Boosting often uses deeper trees.\n",
        "- Sensitivity: AdaBoost is more sensitive to noise; Gradient Boosting's learning rate makes it more robust to outliers.\n",
        "\n",
        "Question 3: How does regularization help in XGBoost?\n",
        "\n",
        "Answer:\n",
        "- Regularization in XGBoost acts as a crucial \"inner voice of restraint\" that prevents the model from becoming too complex and over-fitting to the training data.\n",
        "\n",
        "**XGBoost includes two types of regularization:**\n",
        "\n",
        "1. L1 Regularization (Lasso)\n",
        "  - Adds a penalty for the number of leaf nodes and feature weights\n",
        "  - Encourages sparsity (pushes small weights to zero)\n",
        "  - Helps remove irrelevant features automatically\n",
        "2. L2 Regularization (Ridge)\n",
        "  - Adds penalty for large leaf weights\n",
        "  - Smooths the model and reduces variance\n",
        "\n",
        "**How it Improves the Model**\n",
        "\n",
        "Regularization in XGBoost:\n",
        "-  Prevents the model from growing too deep or complex\n",
        "-  Reduces variance and overfitting\n",
        "- Makes the predictions more stable\n",
        "- Helps handle noisy datasets better\n",
        "\n",
        "**Additional Controls**\n",
        "\n",
        "XGBoost also uses structural regularization parameters like:\n",
        "  - max_depth\n",
        "  - min_child_weight\n",
        "  - gamma\n",
        "  - subsample\n",
        "  - colsample_bytree\n",
        "\n",
        "Question 4: Why is CatBoost considered efficient for handling categorical data?\n",
        "\n",
        "Answer:\n",
        "- CatBoost is considered efficient for handling categorical data because it uses built-in encoding techniques that transform categorical features into numerical values without requiring manual preprocessing such as one-hot encoding or label encoding.\n",
        "\n",
        "**Key reasons for its efficiency include:**\n",
        "- Ordered Target Encoding: Instead of using the entire dataset to calculate statistics for encoding (which causes data leakage), CatBoost sorts data randomly and computes category statistics based only on historical observations (past rows). This technique, called Ordered Target Statistics, generates numerical representations that prevent \"target leakage\" and \"prediction shift\".\n",
        "- Native Handling of Categorical Features: CatBoost does not require preprocessing steps like one-hot encoding or label encoding. Users can simply designate categorical columns, and the algorithm handles them directly, which saves time, reduces memory usage, and prevents the creation of sparse, high-dimensional matrices.\n",
        "- Handling High-Cardinality Data: CatBoost is particularly effective at managing categorical features with many unique values (high cardinality), such as user IDs or product codes, by learning efficient numerical representations during training.\n",
        "- Symmetric (Oblivious) Trees: CatBoost uses symmetric trees, where all nodes at the same depth split on the same feature and threshold. This structure acts as a regularizer, reduces overfitting, and allows for faster training and prediction, especially on GPUs.\n",
        "- Combination of Categorical Features: CatBoost automatically creates combinations of categorical features to capture complex interactions, improving accuracy.\n",
        "- Automatic Handling of Missing Values: CatBoost treats missing values as a separate category, allowing the model to learn from them rather than requiring imputation.\n",
        "\n",
        "Question 5: What are some real-world applications where boosting techniques are\n",
        "preferred over bagging methods?\n",
        "\n",
        "Answer:\n",
        "- Boosting techniques are often preferred in applications where high accuracy, fine-grained prediction, and handling complex patterns are important. Boosting reduces bias and works well with tabular/structured data, which makes it popular in many domain-specific machine learning tasks.\n",
        "\n",
        "**Real-World Applications**\n",
        " **Fraud Detection (Banking & Finance)**\n",
        "- Boosting (e.g., XGBoost, LightGBM) is widely used for:\n",
        "  - Credit card fraud detection\n",
        "  - Insurance fraud Because it captures subtle patterns and interactions that bagging methods may miss.\n",
        "\n",
        " **Credit Scoring & Risk Modeling**\n",
        "- Banks use boosting models to predict:\n",
        "  - Loan defaults\n",
        "  - Creditworthiness\n",
        "  - Boosting provides higher precision and interpretability than bagging models like Random Forests.\n",
        "\n",
        " **Online Advertising & Click-Through-Rate (CTR) Predic**tion **bold text**\n",
        "- Boosting is used for:\n",
        "  - CTR prediction\n",
        "  - Ad ranking\n",
        "  - Conversion probability Due to its ability to model nonlinear relationships in sparse featur\n",
        "\n",
        " **Medical Diagnosis & Disease Prediction**\n",
        "- Boosting models are used for:\n",
        "  - Cancer risk classification\n",
        "  - Early diagnosis\n",
        "    Because they improve performance on imbalanced and noisy medical datasets.\n",
        "\n",
        "**Customer Churn Prediction**\n",
        "- Telecom and SaaS companies use boosting to detect which customers are likely to leave, where small improvements in accuracy matter commercially.\n",
        "\n",
        " **Competition & Kaggle-Type Machine Learning**\n",
        "- Boosting dominates many Kaggle competitions involving tabular datasets because it:\n",
        "  -  yields high accuracy\n",
        "  -  handles missing values\n",
        "  - works well with categorical and numerical data\n",
        "\n",
        "**Ranking Problems (Search Engines)**\n",
        "\n",
        "- Search engines like Yandex and Bing have used boosting for:\n",
        "  - Document ranking\n",
        "  - Query scoring\n",
        "\n",
        " **Why Boosting Over Bagging?**\n",
        "\n",
        "| Reason                     | Explanation                               |\n",
        "| -------------------------- | ----------------------------------------- |\n",
        "| Reduces Bias               | Learns complex patterns sequentially      |\n",
        "| Better for Structured Data | Common in finance, healthcare, marketing  |\n",
        "| Handles Imbalanced Data    | Reweights misclassified samples           |\n",
        "| Higher Accuracy            | Performs better when accuracy is critical |\n",
        "\n"
      ],
      "metadata": {
        "id": "jez7tC89PXQk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Question 6: Write a Python program to:\n",
        "● Train an AdaBoost Classifier on the Breast Cancer dataset\n",
        "● Print the model accuracy\n",
        "'''\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "model = AdaBoostClassifier(n_estimators=100, learning_rate=1.0, random_state=42)\n",
        "\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(\"AdaBoost Model Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nnzm9FFsihRR",
        "outputId": "afc69a82-5f9a-4855-da2f-1e7721ee514b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AdaBoost Model Accuracy: 0.9736842105263158\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Question 7: Write a Python program to:\n",
        "● Train a Gradient Boosting Regressor on the California Housing dataset\n",
        "● Evaluate performance using R-squared score\n",
        "'''\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "data = fetch_california_housing()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "model = GradientBoostingRegressor(\n",
        "    n_estimators=200,\n",
        "    learning_rate=0.1,\n",
        "    max_depth=3,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print(\"Gradient Boosting R² Score:\", r2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IDWH3hVsi4vz",
        "outputId": "4ab2b6ec-7e87-4312-b83b-2debfe2648e2"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradient Boosting R² Score: 0.8004451261281281\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Question 8: Write a Python program to:\n",
        "● Train an XGBoost Classifier on the Breast Cancer dataset\n",
        "● Tune the learning rate using GridSearchCV\n",
        "● Print the best parameters and accuracy\n",
        "'''\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "model = XGBClassifier(\n",
        "    n_estimators=200,\n",
        "    max_depth=3,\n",
        "    subsample=1.0,\n",
        "    colsample_bytree=1.0,\n",
        "    random_state=42,\n",
        "    use_label_encoder=False,\n",
        "    eval_metric='logloss'\n",
        ")\n",
        "\n",
        "param_grid = {\n",
        "    'learning_rate': [0.01, 0.05, 0.1, 0.2]\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=model,\n",
        "    param_grid=param_grid,\n",
        "    cv=3,\n",
        "    scoring='accuracy',\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "print(\"Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7iHX1fqxjbzk",
        "outputId": "3e9a2510-dedd-418e-d0b0-ce95670dd628"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'learning_rate': 0.05}\n",
            "Accuracy: 0.956140350877193\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [08:39:07] WARNING: /workspace/src/learner.cc:790: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Question 9: Write a Python program to:\n",
        "● Train a CatBoost Classifier\n",
        "● Plot the confusion matrix using seaborn\n",
        "'''\n",
        "!pip install catboost\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from catboost import CatBoostClassifier\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "model = CatBoostClassifier(verbose=0)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "plt.title(\"CatBoost Confusion Matrix\")\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 826
        },
        "id": "GonXIMgej_RT",
        "outputId": "3b505c9c-565c-4f6d-9506-405cf2192801"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting catboost\n",
            "  Downloading catboost-1.2.8-cp312-cp312-manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.12/dist-packages (from catboost) (0.21)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from catboost) (3.10.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.16.0 in /usr/local/lib/python3.12/dist-packages (from catboost) (2.0.2)\n",
            "Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.12/dist-packages (from catboost) (2.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from catboost) (1.16.3)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.12/dist-packages (from catboost) (5.24.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.12/dist-packages (from catboost) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24->catboost) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24->catboost) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24->catboost) (2025.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (4.61.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (3.3.1)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.12/dist-packages (from plotly->catboost) (9.1.2)\n",
            "Downloading catboost-1.2.8-cp312-cp312-manylinux2014_x86_64.whl (99.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.2/99.2 MB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: catboost\n",
            "Successfully installed catboost-1.2.8\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 600x400 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 10: You're working for a FinTech company trying to predict loan default using customer demographics and transaction behavior.\n",
        "The dataset is imbalanced, contains missing values, and has both numeric and\n",
        "categorical features.\n",
        "\n",
        "**Describe your step-by-step data science pipeline using boosting techniques:**\n",
        "- Data preprocessing & handling missing/categorical values\n",
        "- Choice between AdaBoost, XGBoost, or CatBoost\n",
        "- Hyperparameter tuning strategy\n",
        "- Evaluation metrics you'd choose and why\n",
        "- How the business would benefit from your model\n",
        "\n",
        "**Data Preprocessing & Handling Missing/Categorical Values**\n",
        "- Missing Values: Use advanced imputation like KNNImputer (for mixed types) or IterativeImputer (MICE) for numeric, and Mode Imputation/Category Encoding for categorical features.\n",
        "- Categorical Features: Apply One-Hot Encoding for low-cardinality features or Target Encoding (with smoothing) for high-cardinality features to avoid dimension explosion and preserve information.\n",
        "- Feature Scaling: Scale numeric features using StandardScaler or MinMaxScaler.\n",
        "- Feature Engineering: Create relevant features like Debt-to-Income Ratio (DTI) or Loan-to-Value (LTV).\n",
        "- Handling Imbalance: Use SMOTE (Synthetic Minority Over-sampling Technique) on training data only, or adjust class weights in the model.\n",
        "\n",
        "**Choice Between AdaBoost, XGBoost, CatBoost**\n",
        "- XGBoost: Excellent performance, handles missing values internally, often a strong baseline.\n",
        "- CatBoost: Superior for categorical features (handles them natively without manual encoding), generally robust.\n",
        "- AdaBoost: Simpler, but may struggle with noisy data/outliers compared to XGBoost/CatBoost.\n",
        "- Choice: Start with XGBoost or CatBoost due to built-in handling of mixed data types and high performance; CatBoost is ideal if many categorical features exist.\n",
        "\n",
        "**Hyperparameter Tuning Strategy**\n",
        "- Method: Use RandomizedSearchCV or Bayesian Optimization (e.g., Hyperopt, Optuna) for efficiency over Grid Search.\n",
        "- Key Parameters:\n",
        "  - n_estimators (boosting rounds)\n",
        "  - learning_rate (shrinkage)\n",
        "  - max_depth, min_child_weight (complexity control)\n",
        "  - subsample, colsample_bytree (regularization)\n",
        "  - scale_pos_weight (for imbalance).\n",
        "\n",
        "**4. Evaluation Metrics**\n",
        "- Area Under the Precision-Recall Curve (AUC-PR): Best for imbalanced datasets, focuses on performance of the minority (default) class.\n",
        "- Recall (Sensitivity): How many actual defaults are caught (minimizes false negatives).\n",
        "- Precision: Of predicted defaults, how many are correct (minimizes false positives, less costly).\n",
        "- F1-Score: Harmonic mean of Precision & Recall.\n",
        "\n",
        "**Business Benefit**\n",
        "- Risk Mitigation: Identify high-risk applicants early, reducing loan defaults and financial losses.\n",
        "- Profit Optimization: Approve more good loans while denying risky ones, increasing portfolio quality.\n",
        "- Dynamic Pricing: Offer personalized interest rates based on predicted risk.\n",
        "- Operational Efficiency: Automate loan approvals for low-risk applicants."
      ],
      "metadata": {
        "id": "E5qzrQW0laoN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from catboost import CatBoostClassifier, Pool\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, roc_auc_score\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "np.random.seed(42)\n",
        "num_samples = 1000\n",
        "\n",
        "data = {\n",
        "    'age': np.random.randint(20, 65, num_samples),\n",
        "    'income': np.random.normal(50000, 15000, num_samples).astype(int),\n",
        "    'loan_amount': np.random.normal(10000, 5000, num_samples).astype(int),\n",
        "    'credit_score': np.random.randint(300, 850, num_samples),\n",
        "    'education': np.random.choice(['High School', 'Bachelors', 'Masters', 'PhD'], num_samples),\n",
        "    'employment_status': np.random.choice(['Employed', 'Unemployed', 'Self-Employed'], num_samples),\n",
        "    'loan_type': np.random.choice(['Personal', 'Mortgage', 'Auto'], num_samples),\n",
        "    'default': np.random.choice([0, 1], num_samples, p=[0.85, 0.15])\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "for col in ['income', 'credit_score', 'education']:\n",
        "    df.loc[df.sample(frac=0.05).index, col] = np.nan\n",
        "\n",
        "target = \"default\"\n",
        "categorical_cols = df.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n",
        "numeric_cols = df.select_dtypes(include=[\"int64\", \"float64\"]).columns.tolist()\n",
        "\n",
        "\n",
        "if target in numeric_cols:\n",
        "    numeric_cols.remove(target)\n",
        "\n",
        "for col in categorical_cols:\n",
        "    if df[col].isnull().any():\n",
        "        df[col] = df[col].astype(str).replace('nan', 'NaN')\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    df.drop(columns=[target]),\n",
        "    df[target],\n",
        "    test_size=0.2,\n",
        "    stratify=df[target],\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "train_pool = Pool(X_train, y_train, cat_features=categorical_cols)\n",
        "test_pool = Pool(X_test, y_test, cat_features=categorical_cols)\n",
        "\n",
        "model = CatBoostClassifier(\n",
        "    iterations=500,\n",
        "    learning_rate=0.05,\n",
        "    depth=8,\n",
        "    loss_function='Logloss',\n",
        "    eval_metric='AUC',\n",
        "    class_weights=[1, 5],\n",
        "    verbose=False,\n",
        "    random_seed=42\n",
        ")\n",
        "\n",
        "model.fit(train_pool, eval_set=test_pool, early_stopping_rounds=60)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "y_prob = model.predict_proba(X_test)[:,1]\n",
        "\n",
        "auc = roc_auc_score(y_test, y_prob)\n",
        "print(\"ROC-AUC Score:\", auc)\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aRNXs5HjnmiG",
        "outputId": "43ce4577-519b-44e3-dd31-f9d7f9c027a0"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROC-AUC Score: 0.5672514619883041\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.91      0.88       171\n",
            "           1       0.17      0.10      0.13        29\n",
            "\n",
            "    accuracy                           0.80       200\n",
            "   macro avg       0.51      0.51      0.51       200\n",
            "weighted avg       0.76      0.80      0.77       200\n",
            "\n"
          ]
        }
      ]
    }
  ]
}