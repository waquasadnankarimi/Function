{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMMyZiK9leNk0yZ/jJ8WIuH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/waquasadnankarimi/Function/blob/main/Ensemble_Learning9.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 1: What is Ensemble Learning in machine learning? Explain the key idea\n",
        "behind it.\n",
        "\n",
        "Answer:\n",
        "- Ensemble Learning in machine learning refers to a technique where multiple models (called learners) are combined to solve a problem and achieve better performance than any single model alone.\n",
        "\n",
        "**Key Idea: Wisdom of the Crowd**\n",
        " - **Diversity**: Different models (e.g., decision trees, neural networks) learn different patterns from the data.\n",
        " - **Compensation**: Errors made by one model are often corrected by others in the ensemble, as their individual predictions balance out.\n",
        " - **Aggregation**: A final decision is made by combining their outputs, often through voting (classification) or averaging (regression).\n",
        "\n",
        "**How it Works (Simplified)**\n",
        "\n",
        "- Build Multiple Models: Train several base models (e.g., Decision Trees, Support Vector Machines) on the training data.\n",
        "- Combine Predictions: Aggregate their individual predictions using methods like voting or averaging to get a final, superior prediction.\n",
        "\n",
        "Question 2: What is the difference between Bagging and Boosting?\n",
        "\n",
        "Answer:\n",
        "- Bagging and Boosting are two major ensemble learning techniques, but they differ in how they train models and how they reduce errors,\n",
        "\n",
        "| **Feature**      | **Bagging (Bootstrap Aggregating)**                   | **Boosting**                                                       |\n",
        "| ---------------- | ----------------------------------------------------- | ------------------------------------------------------------------ |\n",
        "| **Training**     | Parallel (independent models)                         | Sequential (each model depends on previous)                        |\n",
        "| **Goal**         | Reduce Variance (avoid overfitting)                   | Reduce Bias (avoid underfitting)                                   |\n",
        "| **Data**         | Random subsets with replacement (bootstrapping)       | Focuses more on misclassified samples of previous models           |\n",
        "| **Model Weight** | All models have equal weight                          | Models weighted by performance (better models have more influence) |\n",
        "| **Approach**     | Aggregate/average predictions of multiple weak models | Iteratively convert weak models into a strong model                |\n",
        "| **Use Case**     | Works well for unstable, high-variance models         | Works well for simple, high-bias models                            |\n",
        "| **Examples**     | Random Forest, Bagged Trees                           | Gradient Boosting, AdaBoost, XGBoost                               |\n",
        "\n",
        "Question 3: What is bootstrap sampling and what role does it play in Bagging methods like Random Forest?\n",
        "\n",
        "Answer:\n",
        "- Bootstrap sampling creates diverse training sets for Bagging (Bootstrap Aggregation) by repeatedly drawing data with replacement, which reduces variance and overfitting.\n",
        "\n",
        "**Role in Bagging (Random Forest)**\n",
        "- **Creates Diversity**: Each decision tree in a Random Forest is trained on a different bootstrap sample, making each tree learn slightly different patterns from the data.\n",
        "- **Reduces Overfitting**: By training on varied subsets, individual trees are less likely to overfit to noise in the data, a common issue with single, deep decision trees.\n",
        "- **Enhances Stability**: Aggregating predictions (majority vote for classification, average for regression) from these diverse, weaker models results in a single, more robust, and accurate final prediction.\n",
        "- **Out-of-Bag (OOB) Error**: The data points not selected in a specific bootstrap sample (the \"out-of-bag\" data) can be used to estimate the model's performance without needing a separate validation set.\n",
        "\n",
        "Question 4: What are Out-of-Bag (OOB) samples and how is OOB score used to\n",
        "evaluate ensemble models?\n",
        "\n",
        "Answer:\n",
        "- Out-of-Bag (OOB) samples are data points left out from the bootstrap sample used to train individual trees in bagging ensembles (like Random Forests).\n",
        "\n",
        "**What are OOB Samples?**\n",
        "- Bootstrap Sampling: Bagging methods train multiple models (e.g., decision trees) by sampling the original dataset with replacement to create slightly different training sets (bootstrap samples) for each model.\n",
        "- Left-Out Data: Because sampling is with replacement, some original data points are never chosen for a specific tree's training set; these are the \"out-of-bag\" (OOB) samples, making up about 37% of the data on average.\n",
        "\n",
        "**How the OOB Score Evaluates Models:**\n",
        "\n",
        "- Individual Tree Prediction: For each OOB sample, identify all the trees in the ensemble that did not see it during their training.\n",
        "- Majority Vote/Average: Use these specific trees to predict the outcome (class or regression value) for that OOB sample.\n",
        "- Calculate Error/Score: Compare these predictions to the sample's true label to find the error (OOB Error) or accuracy (OOB Score) for that sample.\n",
        "- Aggregate: Average the errors or scores across all samples in the dataset to get the final OOB score, a robust, internal validation metric that reflects how well the model generalizes\n",
        "\n",
        "Question 5: Compare feature importance analysis in a single Decision Tree vs. a\n",
        "Random Forest.\n",
        "\n",
        "Answer:\n",
        "- Feature Importance in a Single Decision Tree\n",
        "- In a single Decision Tree, feature importance is determined by how much each feature reduces the impurity (e.g., Gini impurity or entropy) of the nodes it splits [1].\n",
        "  - Calculation: Importance is typically measured as the total reduction in impurity caused by a specific feature across all splits in which it was used [1].\n",
        "- Characteristics:\n",
        "  - Local Importance: The results can be highly unstable and dependent on the specific training data and the tree's structure. A small change in the data can drastically alter which features are selected for the top splits [1].\n",
        "  - Bias Towards High Cardinality: Decision trees inherently favor continuous or high-cardinality categorical variables, as these features offer more splitting possibilities and thus a greater potential for impurity reduction, which can skew the importance scores [1].\n",
        "  - Prone to Overfitting: The importance scores are derived from a potentially overfit model, meaning they might reflect the noise in the training data rather than the true underlying relationships [1]\n",
        "\n",
        "**Feature Importance in a Random Forest**\n",
        "\n",
        "- A Random Forest is an ensemble of many decision trees, so feature importance is computed by:\n",
        "  - Averaging feature contributions across many trees\n",
        "  - Using bootstrap + random feature selection increases diversity\n",
        "  - Producing more reliable and stable rankings\n",
        "\n",
        "| **Feature**           | **Single Decision Tree**                                               | **Random Forest**                                           |\n",
        "| --------------------- | ---------------------------------------------------------------------- | ----------------------------------------------------------- |\n",
        "| **Calculation Basis** | Total impurity reduction from splits within one tree                   | Average impurity reduction across all trees in the ensemble |\n",
        "| **Stability**         | Highly unstable; sensitive to data variations                          | Stable and robust due to aggregation                        |\n",
        "| **Reliability**       | Less reliable; may overfit and capture noise                           | More reliable; provides a global, robust estimate           |\n",
        "| **Bias**              | Strong bias toward features with many unique values (high cardinality) | Bias reduced due to feature randomness and averaging        |\n",
        "\n",
        "\n",
        "- In essence, while both methods use the same underlying metric (impurity reduction), the Random Forest provides a far more robust, stable, and reliable measure of global feature importance by leveraging the \"wisdom of the crowd\" principle [1].\n"
      ],
      "metadata": {
        "id": "pDOE7bAXN80E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Question 6: Write a Python program to:\n",
        "● Load the Breast Cancer dataset using\n",
        "sklearn.datasets.load_breast_cancer()\n",
        "● Train a Random Forest Classifier\n",
        "● Print the top 5 most important features based on feature importance scores.\n",
        "'''\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import numpy as np\n",
        "\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "feature_names = data.feature_names\n",
        "\n",
        "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "model.fit(X, y)\n",
        "\n",
        "importances = model.feature_importances_\n",
        "\n",
        "indices = np.argsort(importances)[::-1][:5]\n",
        "\n",
        "print(\"Top 5 Important Features:\\n\")\n",
        "for i in indices:\n",
        "    print(f\"{feature_names[i]}: {importances[i]:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7TQ-Mw35XS3F",
        "outputId": "010ccfea-2808-4b27-a12c-c0861895275f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 5 Important Features:\n",
            "\n",
            "worst area: 0.1394\n",
            "worst concave points: 0.1322\n",
            "mean concave points: 0.1070\n",
            "worst radius: 0.0828\n",
            "worst perimeter: 0.0808\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Question 7: Write a Python program to:\n",
        "● Load the Breast Cancer dataset using\n",
        "● Train a Random Forest Classifier\n",
        "● Print the top 5 most important features based on feature importance scores.\n",
        "'''\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "dt = DecisionTreeClassifier(random_state=42)\n",
        "dt.fit(X_train, y_train)\n",
        "y_pred_dt = dt.predict(X_test)\n",
        "dt_accuracy = accuracy_score(y_test, y_pred_dt)\n",
        "\n",
        "bagging = BaggingClassifier(\n",
        "    estimator=DecisionTreeClassifier(),\n",
        "    n_estimators=50,\n",
        "    random_state=42\n",
        ")\n",
        "bagging.fit(X_train, y_train)\n",
        "y_pred_bag = bagging.predict(X_test)\n",
        "bagging_accuracy = accuracy_score(y_test, y_pred_bag)\n",
        "\n",
        "print(f\"Decision Tree Accuracy: {dt_accuracy:.4f}\")\n",
        "print(f\"Bagging Classifier Accuracy: {bagging_accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cPnYIfBgXoYi",
        "outputId": "9ef0e76d-c1cf-4a13-c83e-d5cd2801d8b4"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree Accuracy: 1.0000\n",
            "Bagging Classifier Accuracy: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Question 8: Write a Python program to:\n",
        "● Train a Random Forest Classifier\n",
        "● Tune hyperparameters max_depth and n_estimators using GridSearchCV\n",
        "● Print the best parameters and final accuracy\n",
        "'''\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "\n",
        "param_grid = {\n",
        "    \"n_estimators\": [50, 100, 150],\n",
        "    \"max_depth\": [None, 3, 5, 7]\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=rf,\n",
        "    param_grid=param_grid,\n",
        "    cv=5,\n",
        "    scoring='accuracy'\n",
        ")\n",
        "\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Final Test Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a6ruoGMwYpwe",
        "outputId": "ec4c5ed9-2a57-4868-b804-4c5e1bf28f9b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'max_depth': None, 'n_estimators': 100}\n",
            "Final Test Accuracy: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Question 9: Write a Python program to:\n",
        "● Train a Bagging Regressor and a Random Forest Regressor on the California\n",
        "Housing dataset\n",
        "● Compare their Mean Squared Errors (MSE)\n",
        "'''\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import BaggingRegressor, RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "data = fetch_california_housing()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "bagging = BaggingRegressor(\n",
        "    estimator=DecisionTreeRegressor(), # Changed base_estimator to estimator\n",
        "    n_estimators=100,\n",
        "    random_state=42\n",
        ")\n",
        "bagging.fit(X_train, y_train)\n",
        "y_pred_bag = bagging.predict(X_test)\n",
        "mse_bag = mean_squared_error(y_test, y_pred_bag)\n",
        "\n",
        "\n",
        "rf = RandomForestRegressor(\n",
        "    n_estimators=100,\n",
        "    random_state=42\n",
        ")\n",
        "rf.fit(X_train, y_train)\n",
        "y_pred_rf = rf.predict(X_test)\n",
        "mse_rf = mean_squared_error(y_test, y_pred_rf)\n",
        "\n",
        "print(f\"Bagging Regressor MSE: {mse_bag:.4f}\")\n",
        "print(f\"Random Forest Regressor MSE: {mse_rf:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aok1a1JSZZMH",
        "outputId": "f8dca844-c801-46f6-91c0-4daa459b8af7"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bagging Regressor MSE: 0.2559\n",
            "Random Forest Regressor MSE: 0.2554\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Question 10: You are working as a data scientist at a financial institution to predict loan\n",
        "default. You have access to customer demographic and transaction history data.\n",
        "You decide to use ensemble techniques to increase model performance.\n",
        "Explain your step-by-step approach to:\n",
        "● Choose between Bagging or Boosting\n",
        "● Handle overfitting\n",
        "● Select base models\n",
        "● Evaluate performance using cross-validation\n",
        "● Justify how ensemble learning improves decision-making in this real-world\n",
        "context.\n",
        "'''\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score, classification_report\n",
        "\n",
        "X, y = make_classification(\n",
        "    n_samples=1000, n_features=20, n_informative=10, n_redundant=5,\n",
        "    n_clusters_per_class=2, weights=[0.8,0.2], flip_y=0.05, random_state=42\n",
        ")\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, stratify=y, random_state=42\n",
        ")\n",
        "\n",
        "bagging = BaggingClassifier(\n",
        "    estimator=DecisionTreeClassifier(max_depth=5),\n",
        "    n_estimators=50,\n",
        "    random_state=42\n",
        ")\n",
        "bagging.fit(X_train, y_train)\n",
        "y_pred_bag = bagging.predict(X_test)\n",
        "\n",
        "bagging_acc = accuracy_score(y_test, y_pred_bag)\n",
        "bagging_auc = roc_auc_score(y_test, bagging.predict_proba(X_test)[:,1])\n",
        "\n",
        "print(\"=== Bagging Classifier ===\")\n",
        "print(f\"Accuracy: {bagging_acc:.4f}\")\n",
        "print(f\"AUC: {bagging_auc:.4f}\")\n",
        "print(classification_report(y_test, y_pred_bag))\n",
        "\n",
        "gbc = GradientBoostingClassifier(random_state=42)\n",
        "\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 150],\n",
        "    'max_depth': [3, 5],\n",
        "    'learning_rate': [0.05, 0.1, 0.2]\n",
        "}\n",
        "\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "grid = GridSearchCV(gbc, param_grid, cv=cv, scoring='roc_auc', n_jobs=-1)\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "best_gbc = grid.best_estimator_\n",
        "y_pred_gbc = best_gbc.predict(X_test)\n",
        "\n",
        "gbc_acc = accuracy_score(y_test, y_pred_gbc)\n",
        "gbc_auc = roc_auc_score(y_test, best_gbc.predict_proba(X_test)[:,1])\n",
        "\n",
        "print(\"\\n=== Gradient Boosting Classifier ===\")\n",
        "print(f\"Best Parameters: {grid.best_params_}\")\n",
        "print(f\"Accuracy: {gbc_acc:.4f}\")\n",
        "print(f\"AUC: {gbc_auc:.4f}\")\n",
        "print(classification_report(y_test, y_pred_gbc))\n",
        "\n",
        "print(\"\\n=== Comparison ===\")\n",
        "print(f\"Bagging Accuracy: {bagging_acc:.4f}, AUC: {bagging_auc:.4f}\")\n",
        "print(f\"Boosting Accuracy: {gbc_acc:.4f}, AUC: {gbc_auc:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ochrs83QaCRc",
        "outputId": "cda365b5-fa87-4bb6-f479-c26c9cb0635a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Bagging Classifier ===\n",
            "Accuracy: 0.9033\n",
            "AUC: 0.9492\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.89      1.00      0.94       234\n",
            "           1       1.00      0.56      0.72        66\n",
            "\n",
            "    accuracy                           0.90       300\n",
            "   macro avg       0.94      0.78      0.83       300\n",
            "weighted avg       0.91      0.90      0.89       300\n",
            "\n",
            "\n",
            "=== Gradient Boosting Classifier ===\n",
            "Best Parameters: {'learning_rate': 0.2, 'max_depth': 5, 'n_estimators': 150}\n",
            "Accuracy: 0.9267\n",
            "AUC: 0.9573\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      0.99      0.95       234\n",
            "           1       0.96      0.70      0.81        66\n",
            "\n",
            "    accuracy                           0.93       300\n",
            "   macro avg       0.94      0.84      0.88       300\n",
            "weighted avg       0.93      0.93      0.92       300\n",
            "\n",
            "\n",
            "=== Comparison ===\n",
            "Bagging Accuracy: 0.9033, AUC: 0.9492\n",
            "Boosting Accuracy: 0.9267, AUC: 0.9573\n"
          ]
        }
      ]
    }
  ]
}